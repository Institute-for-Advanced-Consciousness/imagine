{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelimenary Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "mne.set_log_level('WARNING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "dir_path = \"G:\\\\IMAGINE Analysis\\\\preprocessed_data\\\\\"\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(dir_path)\n",
    "# all_files = ['IMAGINE_4002_Med2_04262022-cleaned-epo.fif', 'Imagine_Med1_220427-cleaned-epo.fif']\n",
    "\n",
    "# Filter the list to include only .fif files\n",
    "eeg_files = [f for f in all_files if f.endswith('.fif')]\n",
    "\n",
    "# Load each EEG file and store in a dictionary\n",
    "eeg_data_dict = {}\n",
    "for eeg_file in eeg_files:\n",
    "    full_path = os.path.join(dir_path, eeg_file)\n",
    "    epochs = mne.read_epochs(full_path, preload=True)\n",
    "    # Use filename without extension as the dictionary key\n",
    "    key = os.path.splitext(eeg_file)[0]\n",
    "    eeg_data_dict[key] = epochs\n",
    "\n",
    "# Now, eeg_data_dict contains all the loaded EEG data, with the filenames as keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "\n",
    "# cleaned_epochs = eeg_data_dict['IMAGINE_4002_Med2_04262022-cleaned-epo']\n",
    "\n",
    "# cleaned_epochs.plot(n_channels=32, n_epochs=4, scalings={'eeg': 'auto'}, show=False, block=False, use_opengl=True, precompute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directories exist or create them\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def delete_existing_files(directory):\n",
    "    \"\"\"Delete all files in a given directory.\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-wise PSD saved in G:\\IMAGINE Analysis\\psd\\\n",
      "Average PSD saved in G:\\IMAGINE Analysis\\avg_psd\\\n"
     ]
    }
   ],
   "source": [
    "from mne.time_frequency import psd_array_multitaper\n",
    "\n",
    "# Define a dictionary to store the PSD results\n",
    "psd_dict = {}\n",
    "\n",
    "for key, epochs in eeg_data_dict.items():\n",
    "    # Extract data from the epochs. Data shape is (n_epochs, n_channels, n_times)\n",
    "    epoch_data = epochs.get_data()\n",
    "    \n",
    "    # Compute the PSD for each epoch in the file\n",
    "    psds = []\n",
    "    for single_epoch in epoch_data:\n",
    "        psd, freqs = psd_array_multitaper(single_epoch, fmin=0.5, fmax=45., \n",
    "                                          sfreq=epochs.info['sfreq'], verbose=0, n_jobs=-1)\n",
    "        psds.append(psd)\n",
    "    \n",
    "    # Store the computed PSDs in the psd_dict\n",
    "    psd_dict[key] = psds\n",
    "\n",
    "# Define frequency bands\n",
    "bands = {\n",
    "    'Delta': (0.5, 4),\n",
    "    'Theta': (4, 8),\n",
    "    'Alpha': (8, 13),\n",
    "    'Beta': (13, 30),\n",
    "    'Gamma': (30, 45)\n",
    "}\n",
    "\n",
    "dfs = {}  # Dictionary to store dataframes for each file\n",
    "\n",
    "for key, psds in psd_dict.items():\n",
    "    df_list = []\n",
    "    \n",
    "    for epoch_no, single_epoch_psd in enumerate(psds):\n",
    "        for ch_idx, channel_psd in enumerate(single_epoch_psd):\n",
    "            channel_name = eeg_data_dict[key].ch_names[ch_idx]\n",
    "            \n",
    "            for band_name, (fmin, fmax) in bands.items():\n",
    "                # Find frequencies within the desired band\n",
    "                band_mask = np.logical_and(freqs >= fmin, freqs < fmax)\n",
    "                # Average the power values within this band\n",
    "                avg_power = np.mean(channel_psd[band_mask])\n",
    "                # Convert power to dB\n",
    "                power_db = 10 * np.log10(avg_power)\n",
    "                \n",
    "                df_list.append([epoch_no, channel_name, band_name, power_db])\n",
    "    \n",
    "    # Create a DataFrame for the current file and store in dfs dictionary\n",
    "    df = pd.DataFrame(df_list, columns=['Epoch No.', 'Channel', 'FrequencyBand', 'Power_dB'])\n",
    "    dfs[key] = df\n",
    "\n",
    "# Define directories for epoch-wise and average PSD\n",
    "psd_dir = \"G:\\\\IMAGINE Analysis\\\\psd\\\\\"\n",
    "avg_psd_dir = \"G:\\\\IMAGINE Analysis\\\\avg_psd\\\\\"\n",
    "\n",
    "# Ensure directories exist\n",
    "ensure_directory_exists(psd_dir)\n",
    "ensure_directory_exists(avg_psd_dir)\n",
    "\n",
    "# Delete existing files in the directories\n",
    "delete_existing_files(psd_dir)\n",
    "delete_existing_files(avg_psd_dir)\n",
    "\n",
    "dfs_avg = {}  # Dictionary to store average PSD dataframes\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "    \n",
    "    # Save epoch-wise PSD\n",
    "    epoch_psd_file = os.path.join(psd_dir, modified_key + 'PSD.csv')\n",
    "    df.to_csv(epoch_psd_file, index=False)\n",
    "\n",
    "    # Compute and save average PSD\n",
    "    avg_psd_data = df.groupby(['Channel', 'FrequencyBand']).mean().reset_index()\n",
    "    avg_psd_data = avg_psd_data[['Channel', 'FrequencyBand', 'Power_dB']]\n",
    "    \n",
    "    dfs_avg[key] = avg_psd_data\n",
    "    avg_psd_file = os.path.join(avg_psd_dir, modified_key + 'AveragePSD.csv')\n",
    "    avg_psd_data.to_csv(avg_psd_file, index=False)\n",
    "\n",
    "print(\"Epoch-wise PSD saved in\", psd_dir)\n",
    "print(\"Average PSD saved in\", avg_psd_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined average PSD data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_psd.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_session_id(filename):\n",
    "    # Split the filename by underscores and take the relevant parts for the session ID\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        # Construct session ID from parts. E.g., '4002_Med2' or '4019_MED1'\n",
    "        session_id = parts[1] + '_' + parts[2].split('-')[0]\n",
    "        return session_id\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Define the directory containing average PSD files\n",
    "avg_psd_dir = \"G:\\\\IMAGINE Analysis\\\\avg_psd\\\\\"\n",
    "\n",
    "# List all CSV files in the directory\n",
    "avg_psd_files = [f for f in os.listdir(avg_psd_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame for merging\n",
    "merged_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the files and merge them into the DataFrame\n",
    "for file in avg_psd_files:\n",
    "    file_path = os.path.join(avg_psd_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract session ID from the filename\n",
    "    session_id = extract_session_id(file)\n",
    "    df.rename(columns={'Power_dB': session_id}, inplace=True)\n",
    "\n",
    "    if merged_df.empty:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, df, on=['Channel', 'FrequencyBand'], how='outer')\n",
    "\n",
    "# Calculate average for each session and append as a new row\n",
    "average_row = {'Channel': 'Average', 'FrequencyBand': 'Average'}\n",
    "for col in merged_df.columns[2:]:  # Skipping the first two columns ('Channel', 'FrequencyBand')\n",
    "    average_row[col] = merged_df[col].mean()\n",
    "\n",
    "average_df = pd.DataFrame([average_row])\n",
    "merged_df = pd.concat([merged_df, average_df], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "combined_csv_dir = \"G:\\\\IMAGINE Analysis\\\\combined_results\"\n",
    "ensure_directory_exists(combined_csv_dir)\n",
    "combined_psd_path = os.path.join(combined_csv_dir, 'combined_avg_psd.csv')\n",
    "merged_df.to_csv(combined_psd_path, index=False)\n",
    "\n",
    "print(f\"Combined average PSD data saved in {combined_psd_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All connectivity, modularity, and entropy measures, and their averages saved to separate CSV files!\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "import community as community_louvain\n",
    "\n",
    "# Define frequency bands\n",
    "bands = {\n",
    "    'Delta': (1, 4),\n",
    "    'Theta': (4, 8),\n",
    "    'Alpha': (8, 13),\n",
    "    'Beta': (13, 30),\n",
    "    'Gamma': (30, 45)\n",
    "}\n",
    "\n",
    "def compute_connectivity(data, sfreq=500, fmin=None, fmax=None, method='coh'):\n",
    "    \"\"\"\n",
    "    Computes connectivity using mne_connectivity.spectral_connectivity_epochs function.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Array-like, shape=(n_epochs, n_signals, n_times) or Epochs object with the EEG data.\n",
    "    - sfreq: The sampling frequency. Required if data is not an Epochs object.\n",
    "    - fmin: Minimum frequency of interest.\n",
    "    - fmax: Maximum frequency of interest.\n",
    "    - method: Connectivity measure to compute (default is 'coh' for coherence).\n",
    "\n",
    "    Returns:\n",
    "    - con_matrix: Connectivity matrix of shape (n_channels, n_channels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute spectral connectivity and average over the frequency dimension\n",
    "    con = spectral_connectivity_epochs(\n",
    "        data=data,\n",
    "        method=method,\n",
    "        sfreq=sfreq,\n",
    "        fmin=fmin,\n",
    "        fmax=fmax,\n",
    "        faverage=True,  # Average over the frequency dimension\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Extract the data from the SpectralConnectivity object\n",
    "    con_data = con.get_data()\n",
    "    \n",
    "    # Since we have set faverage=True, the shape would be (n_signals ** 2,)\n",
    "    n_channels = int(np.sqrt(con_data.shape[0]))\n",
    "    con_matrix = con_data.reshape((n_channels, n_channels))\n",
    "\n",
    "    # Make the matrix symmetric by copying the lower triangular part to the upper triangular part\n",
    "    con_matrix = np.tril(con_matrix) + np.tril(con_matrix, -1).T\n",
    "\n",
    "    return con_matrix\n",
    "\n",
    "def compute_degree_sequence(matrix):\n",
    "    \"\"\"Compute the degree sequence of a connectivity matrix.\"\"\"\n",
    "    return np.sum(matrix, axis=1)\n",
    "\n",
    "def compute_shannon_entropy(distribution):\n",
    "    \"\"\"Compute the Shannon entropy of a distribution.\"\"\"\n",
    "    p_values = distribution / np.sum(distribution)\n",
    "    p_values = p_values[p_values > 0]\n",
    "    return -np.sum(p_values * np.log(p_values))\n",
    "\n",
    "def weighted_modularity(con_matrix):\n",
    "    \"\"\"\n",
    "    Calculate weighted modularity for a given connectivity matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - con_matrix: Weighted connectivity matrix.\n",
    "\n",
    "    Returns:\n",
    "    - modularity: Weighted modularity score.\n",
    "    \"\"\"\n",
    "    # Create a weighted graph from the connectivity matrix\n",
    "    weighted_graph = nx.Graph(con_matrix)\n",
    "\n",
    "    # Compute the best partition using Louvain method (adapted for weighted networks)\n",
    "    partition = community_louvain.best_partition(weighted_graph, weight='weight')\n",
    "    modularity = community_louvain.modularity(partition, weighted_graph, weight='weight')\n",
    "\n",
    "    return modularity\n",
    "\n",
    "def compute_measures(epochs, bands):\n",
    "    \"\"\"\n",
    "    Compute connectivity, modularity, and Shannon entropy for each frequency band and epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - epochs: Epochs object containing the EEG data.\n",
    "    - bands: Dictionary of frequency bands.\n",
    "\n",
    "    Returns:\n",
    "    - df_modularity: DataFrame containing modularity measures.\n",
    "    - df_entropy: DataFrame containing Shannon entropy measures.\n",
    "    - df_conn: DataFrame containing connectivity measures.\n",
    "    \"\"\"\n",
    "    epoch_data = epochs.get_data()\n",
    "    modularity_results = []\n",
    "    entropy_results = []\n",
    "    conn_results = []\n",
    "\n",
    "    for band_name, (fmin, fmax) in bands.items():\n",
    "        for epoch_no, single_epoch in enumerate(epoch_data):\n",
    "            # Compute connectivity matrix\n",
    "            con_matrix = compute_connectivity(\n",
    "                data=np.array([single_epoch]),\n",
    "                sfreq=epochs.info['sfreq'],\n",
    "                fmin=fmin,\n",
    "                fmax=fmax,\n",
    "                method='coh'\n",
    "            )\n",
    "\n",
    "            # Compute degree sequence and Shannon entropy\n",
    "            degree_dist = compute_degree_sequence(con_matrix)\n",
    "            shannon_entropy = compute_shannon_entropy(degree_dist)\n",
    "\n",
    "            # Compute modularity\n",
    "            modularity = weighted_modularity(con_matrix)\n",
    "\n",
    "            # Append modularity and entropy results to their respective lists\n",
    "            modularity_results.append({\n",
    "                'Epoch No.': epoch_no,\n",
    "                'FrequencyBand': band_name,\n",
    "                'Modularity': modularity\n",
    "            })\n",
    "\n",
    "            entropy_results.append({\n",
    "                'Epoch No.': epoch_no,\n",
    "                'FrequencyBand': band_name,\n",
    "                'ShannonEntropy': shannon_entropy\n",
    "            })\n",
    "\n",
    "            # Append connectivity information to conn_results\n",
    "            for i, ch1 in enumerate(epochs.ch_names):\n",
    "                for j, ch2 in enumerate(epochs.ch_names):\n",
    "                    if i <= j:  # To avoid duplication, consider only upper triangular matrix including the diagonal\n",
    "                        conn_results.append({\n",
    "                            'Epoch No.': epoch_no,\n",
    "                            'Ch. Pair': (ch1, ch2),\n",
    "                            'FrequencyBand': band_name,\n",
    "                            'Connectivity': con_matrix[i, j]\n",
    "                        })\n",
    "\n",
    "    # Convert results to DataFrames\n",
    "    df_modularity = pd.DataFrame(modularity_results)\n",
    "    df_entropy = pd.DataFrame(entropy_results)\n",
    "    df_conn = pd.DataFrame(conn_results)\n",
    "\n",
    "    return df_modularity, df_entropy, df_conn\n",
    "\n",
    "# Ensure output directories exist or create them\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def delete_existing_files(directory):\n",
    "    \"\"\"Delete all files in a given directory.\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
    "\n",
    "# Directory setup\n",
    "output_dirs = {\n",
    "    \"modularity\": \"G:\\\\IMAGINE Analysis\\\\modularity\\\\\",\n",
    "    \"entropy\": \"G:\\\\IMAGINE Analysis\\\\shannon_entropy\\\\\",\n",
    "    \"connectivity\": \"G:\\\\IMAGINE Analysis\\\\connectivity\\\\\",\n",
    "    \"avg_modularity\": \"G:\\\\IMAGINE Analysis\\\\avg_modularity\\\\\",\n",
    "    \"avg_entropy\": \"G:\\\\IMAGINE Analysis\\\\avg_shannon_entropy\\\\\"\n",
    "}\n",
    "\n",
    "# Ensure all directories exist\n",
    "for directory in output_dirs.values():\n",
    "    ensure_directory_exists(directory)\n",
    "    \n",
    "for directory in output_dirs.values():\n",
    "    delete_existing_files(directory)\n",
    "\n",
    "# Main computation and saving routine\n",
    "dfs_modularity = {}\n",
    "dfs_entropy = {}\n",
    "dfs_conn = {}\n",
    "\n",
    "for key, epochs in eeg_data_dict.items():\n",
    "    df_modularity, df_entropy, df_conn = compute_measures(epochs, bands)\n",
    "    dfs_modularity[key] = df_modularity\n",
    "    dfs_entropy[key] = df_entropy\n",
    "    dfs_conn[key] = df_conn\n",
    "\n",
    "    # Save modularity, entropy, and connectivity measures\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "    dfs_modularity[key].to_csv(os.path.join(output_dirs[\"modularity\"], modified_key + 'Modularity.csv'), index=False)\n",
    "    dfs_entropy[key].to_csv(os.path.join(output_dirs[\"entropy\"], modified_key + 'ShannonEntropy.csv'), index=False)\n",
    "    dfs_conn[key].to_csv(os.path.join(output_dirs[\"connectivity\"], modified_key + 'Connectivity.csv'), index=False)\n",
    "\n",
    "# Compute and save average measures\n",
    "for key in dfs_modularity.keys():\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "    df_modularity = dfs_modularity[key]\n",
    "    df_entropy = dfs_entropy[key]\n",
    "\n",
    "    # Calculate and save average modularity\n",
    "    avg_modularity_list = [{\n",
    "        'FrequencyBand': band_name,\n",
    "        'AverageModularity': df_modularity[df_modularity['FrequencyBand'] == band_name]['Modularity'].mean()\n",
    "    } for band_name in bands.keys()]\n",
    "    pd.DataFrame(avg_modularity_list).to_csv(os.path.join(output_dirs[\"avg_modularity\"], modified_key + 'AverageModularity.csv'), index=False)\n",
    "\n",
    "    # Calculate and save average entropy\n",
    "    avg_entropy_list = [{\n",
    "        'FrequencyBand': band_name,\n",
    "        'AverageShannonEntropy': df_entropy[df_entropy['FrequencyBand'] == band_name]['ShannonEntropy'].mean()\n",
    "    } for band_name in bands.keys()]\n",
    "    pd.DataFrame(avg_entropy_list).to_csv(os.path.join(output_dirs[\"avg_entropy\"], modified_key + 'AverageShannonEntropy.csv'), index=False)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Directory to save the average connectivity\n",
    "avg_connectivity_dir = \"G:\\\\IMAGINE Analysis\\\\avg_connectivity\\\\\"\n",
    "ensure_directory_exists(avg_connectivity_dir)\n",
    "\n",
    "def process_key(key):\n",
    "    df_conn = dfs_conn[key]\n",
    "    avg_conn_results = []\n",
    "\n",
    "    for band_name in bands.keys():\n",
    "        # Filter the DataFrame for the current band\n",
    "        df_band = df_conn[df_conn['FrequencyBand'] == band_name]\n",
    "\n",
    "        # Iterate over each channel pair\n",
    "        for (ch1, ch2) in set(df_band['Ch. Pair']):\n",
    "            # Filter the DataFrame for the current channel pair\n",
    "            df_pair = df_band[df_band['Ch. Pair'] == (ch1, ch2)]\n",
    "\n",
    "            # Calculate the average connectivity for this band and channel pair\n",
    "            avg_connectivity = df_pair['Connectivity'].mean()\n",
    "\n",
    "            # Append the result\n",
    "            avg_conn_results.append({\n",
    "                'Ch. Pair': (ch1, ch2),\n",
    "                'FrequencyBand': band_name,\n",
    "                'AverageConnectivity': avg_connectivity\n",
    "            })\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    df_avg_conn = pd.DataFrame(avg_conn_results)\n",
    "\n",
    "    # Save to CSV\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "    df_avg_conn.to_csv(os.path.join(avg_connectivity_dir, modified_key + 'AverageConnectivity.csv'), index=False)\n",
    "\n",
    "# Parallel execution\n",
    "Parallel(n_jobs=-1)(delayed(process_key)(key) for key in dfs_conn.keys())\n",
    "\n",
    "print(\"All connectivity, modularity, and entropy measures, and their averages saved to separate CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined average Connectivity data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_Connectivity.csv\n",
      "Combined average Modularity data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_Modularity.csv\n",
      "Combined average ShannonEntropy data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_ShannonEntropy.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_session_id(filename):\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        session_id = parts[1] + '_' + parts[2].split('-')[0]\n",
    "        return session_id\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def combine_average_connectivity(input_dir, output_dir):\n",
    "    avg_files = [f for f in os.listdir(input_dir) if f.endswith('AverageConnectivity.csv')]\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for file in avg_files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        session_id = extract_session_id(file)\n",
    "        df.rename(columns={'AverageConnectivity': session_id}, inplace=True)\n",
    "\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on=['Ch. Pair', 'FrequencyBand'], how='outer')\n",
    "\n",
    "    # Calculate average for each session and append as a new row\n",
    "    average_row = {'Ch. Pair': 'Average', 'FrequencyBand': 'Average'}\n",
    "    for col in merged_df.columns[2:]:\n",
    "        average_row[col] = merged_df[col].mean()\n",
    "\n",
    "    average_df = pd.DataFrame([average_row])\n",
    "    merged_df = pd.concat([merged_df, average_df], ignore_index=True)\n",
    "\n",
    "    combined_csv_path = os.path.join(output_dir, 'combined_avg_Connectivity.csv')\n",
    "    merged_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined average Connectivity data saved in {combined_csv_path}\")\n",
    "\n",
    "def combine_average_measure(input_dir, output_dir, measure_name, merge_on):\n",
    "    avg_files = [f for f in os.listdir(input_dir) if f.endswith('Average' + measure_name + '.csv')]\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for file in avg_files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        session_id = extract_session_id(file)\n",
    "        df.rename(columns={'Average' + measure_name: session_id}, inplace=True)\n",
    "\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on=merge_on, how='outer')\n",
    "\n",
    "    # Calculate average for each session and append as a new row\n",
    "    average_row = {merge_on[0]: 'Average'}\n",
    "    for col in merged_df.columns[1:]:\n",
    "        average_row[col] = merged_df[col].mean()\n",
    "\n",
    "    average_df = pd.DataFrame([average_row])\n",
    "    merged_df = pd.concat([merged_df, average_df], ignore_index=True)\n",
    "\n",
    "    combined_csv_path = os.path.join(output_dir, 'combined_avg_' + measure_name + '.csv')\n",
    "    merged_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined average {measure_name} data saved in {combined_csv_path}\")\n",
    "\n",
    "# Directory paths\n",
    "input_connectivity_dir = \"G:\\\\IMAGINE Analysis\\\\avg_connectivity\\\\\"\n",
    "input_modularity_dir = \"G:\\\\IMAGINE Analysis\\\\avg_modularity\\\\\"\n",
    "input_entropy_dir = \"G:\\\\IMAGINE Analysis\\\\avg_shannon_entropy\\\\\"\n",
    "output_combined_dir = \"G:\\\\IMAGINE Analysis\\\\combined_results\\\\\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_combined_dir):\n",
    "    os.makedirs(output_combined_dir)\n",
    "\n",
    "# Combining data\n",
    "combine_average_connectivity(input_connectivity_dir, output_combined_dir)\n",
    "combine_average_measure(input_modularity_dir, output_combined_dir, 'Modularity', ['FrequencyBand'])\n",
    "combine_average_measure(input_entropy_dir, output_combined_dir, 'ShannonEntropy', ['FrequencyBand'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-wise sample entropy saved to G:\\IMAGINE Analysis\\sample_entropy\\\n",
      "Average sample entropy saved to G:\\IMAGINE Analysis\\avg_sample_entropy\\\n"
     ]
    }
   ],
   "source": [
    "import nolds\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_sampen_for_channel(epoch_no, channel_data, ch_name):\n",
    "    \"\"\"\n",
    "    Compute sample entropy for a single channel.\n",
    "\n",
    "    Parameters:\n",
    "    - epoch_no: The epoch number.\n",
    "    - channel_data: The data for a single channel.\n",
    "    - ch_name: The name of the channel.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with epoch number, channel name, and sample entropy.\n",
    "    \"\"\"\n",
    "    sampen = nolds.sampen(channel_data)\n",
    "    return {'Epoch No.': epoch_no, 'Channel': ch_name, 'SampleEntropy': sampen}\n",
    "\n",
    "def compute_sample_entropy(epochs, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Compute sample entropy for each epoch and channel using parallel processing.\n",
    "\n",
    "    Parameters:\n",
    "    - epochs: Epochs object containing the EEG data.\n",
    "    - n_jobs: The number of jobs to run in parallel (default is -1, which uses all processors).\n",
    "\n",
    "    Returns:\n",
    "    - df_sampen: DataFrame containing sample entropy measures.\n",
    "    \"\"\"\n",
    "    epoch_data = epochs.get_data()\n",
    "\n",
    "    # Parallel computation\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(compute_sampen_for_channel)(epoch_no, channel_data, epochs.ch_names[ch_idx])\n",
    "                                      for epoch_no, epoch in enumerate(epoch_data)\n",
    "                                      for ch_idx, channel_data in enumerate(epoch))\n",
    "\n",
    "    df_sampen = pd.DataFrame(results)\n",
    "    return df_sampen\n",
    "\n",
    "# Assuming eeg_data_dict is your dictionary of EEG data\n",
    "dfs_sampen = {}\n",
    "\n",
    "for key, epochs in eeg_data_dict.items():\n",
    "    df_sampen = compute_sample_entropy(epochs)\n",
    "    dfs_sampen[key] = df_sampen\n",
    "\n",
    "# Directory paths\n",
    "sample_entropy_dir = \"G:\\\\IMAGINE Analysis\\\\sample_entropy\\\\\"\n",
    "avg_sample_entropy_dir = \"G:\\\\IMAGINE Analysis\\\\avg_sample_entropy\\\\\"\n",
    "\n",
    "# Ensure directories exist\n",
    "ensure_directory_exists(sample_entropy_dir)\n",
    "ensure_directory_exists(avg_sample_entropy_dir)\n",
    "\n",
    "# Clear existing files in the directories\n",
    "delete_existing_files(sample_entropy_dir)\n",
    "delete_existing_files(avg_sample_entropy_dir)\n",
    "\n",
    "for key, df_sampen in dfs_sampen.items():\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "\n",
    "    # Save epoch-wise sample entropy in the sample_entropy directory\n",
    "    epoch_wise_file = os.path.join(sample_entropy_dir, modified_key + 'SampleEntropy.csv')\n",
    "    df_sampen.to_csv(epoch_wise_file, index=False)\n",
    "\n",
    "    # Calculate and save average sample entropy in the avg_sample_entropy directory\n",
    "    avg_sampen_list = [{\n",
    "        'Channel': ch,\n",
    "        'AverageSampleEntropy': df_sampen[df_sampen['Channel'] == ch]['SampleEntropy'].mean()\n",
    "    } for ch in epochs.ch_names]\n",
    "\n",
    "    avg_sampen_file = os.path.join(avg_sample_entropy_dir, modified_key + 'AverageSampleEntropy.csv')\n",
    "    pd.DataFrame(avg_sampen_list).to_csv(avg_sampen_file, index=False)\n",
    "\n",
    "print(\"Epoch-wise sample entropy saved to\", sample_entropy_dir)\n",
    "print(\"Average sample entropy saved to\", avg_sample_entropy_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined average Sample Entropy data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_SampleEntropy.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_average_sample_entropy(input_dir, output_dir):\n",
    "    avg_files = [f for f in os.listdir(input_dir) if f.endswith('AverageSampleEntropy.csv')]\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for file in avg_files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        session_id = extract_session_id(file)\n",
    "        df.rename(columns={'AverageSampleEntropy': session_id}, inplace=True)\n",
    "\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='Channel', how='outer')\n",
    "\n",
    "    # Calculate average for each session and append as a new row\n",
    "    average_row = {'Channel': 'Average'}\n",
    "    for col in merged_df.columns[1:]:\n",
    "        average_row[col] = merged_df[col].mean()\n",
    "\n",
    "    average_df = pd.DataFrame([average_row])\n",
    "    merged_df = pd.concat([merged_df, average_df], ignore_index=True)\n",
    "\n",
    "    combined_csv_path = os.path.join(output_dir, 'combined_avg_SampleEntropy.csv')\n",
    "    merged_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined average Sample Entropy data saved in {combined_csv_path}\")\n",
    "\n",
    "# Directory paths\n",
    "input_sample_entropy_dir = \"G:\\\\IMAGINE Analysis\\\\avg_sample_entropy\\\\\"\n",
    "output_combined_dir = \"G:\\\\IMAGINE Analysis\\\\combined_results\\\\\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_combined_dir):\n",
    "    os.makedirs(output_combined_dir)\n",
    "\n",
    "# Combining data\n",
    "combine_average_sample_entropy(input_sample_entropy_dir, output_combined_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fractal Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All detailed and average Higuchi Fractal Dimension measures saved to respective CSV files!\n"
     ]
    }
   ],
   "source": [
    "import antropy as ant\n",
    "\n",
    "def compute_hfd_for_epochs(epochs):\n",
    "    \"\"\"\n",
    "    Computes HFD for each channel in each epoch using the antropy library.\n",
    "    \"\"\"\n",
    "    epoch_data = epochs.get_data()\n",
    "    hfd_results = []\n",
    "\n",
    "    for epoch_no, epoch in enumerate(epoch_data):\n",
    "        for ch_idx, channel in enumerate(epoch):\n",
    "            hfd = ant.higuchi_fd(channel)\n",
    "            hfd_results.append({\n",
    "                'Epoch No.': epoch_no,\n",
    "                'Channel': epochs.ch_names[ch_idx],\n",
    "                'HFD': hfd\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(hfd_results)\n",
    "# Assuming eeg_data_dict is a dictionary of Epochs objects\n",
    "dfs_hfd = {}\n",
    "dfs_hfd_avg = {}\n",
    "\n",
    "for key, epochs in eeg_data_dict.items():\n",
    "    df_hfd = compute_hfd_for_epochs(epochs)\n",
    "    dfs_hfd[key] = df_hfd\n",
    "\n",
    "    # Calculate the average HFD for each channel\n",
    "    avg_hfd = df_hfd.groupby('Channel')['HFD'].mean().reset_index()\n",
    "    avg_hfd.rename(columns={'HFD': 'Average HFD'}, inplace=True)\n",
    "    dfs_hfd_avg[key] = avg_hfd\n",
    "\n",
    "output_hfd_dir = \"G:\\\\IMAGINE Analysis\\\\hfd\\\\\"\n",
    "output_hfd_avg_dir = \"G:\\\\IMAGINE Analysis\\\\hfd_avg\\\\\"  # New directory for average HFD\n",
    "\n",
    "ensure_directory_exists(output_hfd_dir)\n",
    "ensure_directory_exists(output_hfd_avg_dir)\n",
    "\n",
    "# Directories to clean\n",
    "directories = [output_hfd_dir, output_hfd_avg_dir]\n",
    "\n",
    "# Delete existing files in each directory\n",
    "for directory in directories:\n",
    "    delete_existing_files(directory)\n",
    "    \n",
    "for key in dfs_hfd.keys():\n",
    "    modified_key = key.replace('cleaned-epo', '')\n",
    "\n",
    "    # Save detailed HFD results\n",
    "    hfd_file = os.path.join(output_hfd_dir, modified_key + 'HFD.csv')\n",
    "    dfs_hfd[key].to_csv(hfd_file, index=False)\n",
    "\n",
    "    # Save average HFD results in the new directory\n",
    "    avg_hfd_file = os.path.join(output_hfd_avg_dir, modified_key + 'AverageHFD.csv')\n",
    "    dfs_hfd_avg[key].to_csv(avg_hfd_file, index=False)\n",
    "\n",
    "print(\"All detailed and average Higuchi Fractal Dimension measures saved to respective CSV files!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined average HFD data saved in G:\\IMAGINE Analysis\\combined_results\\combined_avg_HFD.csv\n"
     ]
    }
   ],
   "source": [
    "def combine_average_hfd(input_dir, output_dir):\n",
    "    avg_files = [f for f in os.listdir(input_dir) if f.endswith('AverageHFD.csv')]\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for file in avg_files:\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        session_id = extract_session_id(file)\n",
    "        df.rename(columns={'Average HFD': session_id}, inplace=True)\n",
    "\n",
    "        if merged_df.empty:\n",
    "            merged_df = df\n",
    "        else:\n",
    "            merged_df = pd.merge(merged_df, df, on='Channel', how='outer')\n",
    "\n",
    "    # Calculate average for each session and append as a new row\n",
    "    average_row = {'Channel': 'Average'}\n",
    "    for col in merged_df.columns[1:]:\n",
    "        average_row[col] = merged_df[col].mean()\n",
    "\n",
    "    average_df = pd.DataFrame([average_row])\n",
    "    merged_df = pd.concat([merged_df, average_df], ignore_index=True)\n",
    "\n",
    "    combined_csv_path = os.path.join(output_dir, 'combined_avg_HFD.csv')\n",
    "    merged_df.to_csv(combined_csv_path, index=False)\n",
    "    print(f\"Combined average HFD data saved in {combined_csv_path}\")\n",
    "\n",
    "# Directory paths\n",
    "input_hfd_avg_dir = \"G:\\\\IMAGINE Analysis\\\\hfd_avg\\\\\"\n",
    "output_combined_dir = \"G:\\\\IMAGINE Analysis\\\\combined_results\\\\\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "if not os.path.exists(output_combined_dir):\n",
    "    os.makedirs(output_combined_dir)\n",
    "\n",
    "# Combining data\n",
    "combine_average_hfd(input_hfd_avg_dir, output_combined_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
